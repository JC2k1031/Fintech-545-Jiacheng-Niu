{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "144d8b5c-423b-4077-8972-0abc9812c889",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unexpected EOF while parsing (1742035362.py, line 50)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_6998/1742035362.py\"\u001b[0;36m, line \u001b[0;32m50\u001b[0m\n\u001b[0;31m    samples = np.dot(eigenvectors[:, :num_components], np.sqrt(eigenvalues[:num_components])\u001b[0m\n\u001b[0m                                                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unexpected EOF while parsing\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def simulate_pca(a, nsim, nval=None):\n",
    "    # Eigenvalue decomposition\n",
    "    vals, vecs = np.linalg.eigh(a)\n",
    "\n",
    "    # sort values in descending order and corresponding vectors\n",
    "    sorted_index = np.argsort(vals)[::-1]\n",
    "    vals = vals[sorted_index]\n",
    "    vecs = vecs[:, sorted_index]\n",
    "\n",
    "    tv = sum(vals)\n",
    "\n",
    "    posv = np.where(vals >= 1e-8)[0]\n",
    "    if nval is not None:\n",
    "        if nval < posv.shape[0]:\n",
    "            posv = posv[:nval]\n",
    "       \n",
    "    vals = vals[posv]\n",
    "    vecs = vecs[:, posv]\n",
    "\n",
    "    print(f\"Simulating with {posv.shape[0]} PC Factors: {sum(vals) / tv * 100}% total variance explained\")\n",
    "    B = vecs * np.diag(np.sqrt(vals))\n",
    "\n",
    "    m = vals.shape[0]\n",
    "    r = np.random.randn(m, nsim)\n",
    "\n",
    "    return (B @ r).T\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def multivariate_normal_simulation_pca(mean, cov, n_samples, explained_variance=None):\n",
    "    # Perform PCA on the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eigh(cov)\n",
    "\n",
    "    # Sort the eigenvectors in decreasing order of eigenvalues\n",
    "    idx = np.argsort(eigenvalues)[::-1]\n",
    "    eigenvalues = eigenvalues[idx]\n",
    "    eigenvectors = eigenvectors[:, idx]\n",
    "\n",
    "    # Determine the number of components to include based on the explained_variance\n",
    "    if explained_variance is not None:\n",
    "        explained_variance = explained_variance / 100\n",
    "        cum_variance = np.cumsum(eigenvalues) / np.sum(eigenvalues)\n",
    "        num_components = np.argmax(cum_variance >= explained_variance) + 1\n",
    "    else:\n",
    "        num_components = len(eigenvalues)\n",
    "\n",
    "    # Use the top num_components eigenvectors to transform the standard normal samples\n",
    "    samples = np.random.normal(size=(num_components, n_samples))\n",
    "    samples = np.dot(eigenvectors[:, :num_components], np.sqrt(eigenvalues[:num_components])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9af5d87d-e610-4c09-885d-1ee8b0b4058f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def generate_correlation_matrix_variance_vector(data):\n",
    "\n",
    "    # Calculate the covariance matrix\n",
    "    cov_matrix = np.cov(data, rowvar=False)\n",
    "\n",
    "    # Calculate the standard deviation vector\n",
    "    std_vector = np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "    # Calculate the correlation matrix\n",
    "    corr_matrix = np.zeros_like(cov_matrix)\n",
    "    for i in range(cov_matrix.shape[0]):\n",
    "        for j in range(cov_matrix.shape[1]):\n",
    "            corr_matrix[i, j] = cov_matrix[i, j] / (std_vector[i] * std_vector[j])\n",
    "\n",
    "    return corr_matrix, np.diag(cov_matrix)\n",
    "\n",
    "\n",
    "def generate_ew_correlation_matrix_variance_vector(data, lambda_value=0.97):\n",
    "    # Calculate the weighted covariance matrix\n",
    "    weight = lambda_value ** np.arange(data.shape[0])\n",
    "    cov_matrix = np.cov(data, rowvar=False, aweights=weight)\n",
    "\n",
    "    # Calculate the standard deviation vector\n",
    "    std_vector = np.sqrt(np.diag(cov_matrix))\n",
    "\n",
    "    # Calculate the weighted correlation matrix\n",
    "    corr_matrix = np.zeros_like(cov_matrix)\n",
    "    for i in range(cov_matrix.shape[0]):\n",
    "        for j in range(cov_matrix.shape[1]):\n",
    "            corr_matrix[i, j] = cov_matrix[i, j] / (std_vector[i] * std_vector[j])\n",
    "\n",
    "    return corr_matrix, np.diag(cov_matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6abe2ce-b2f8-4919-8fe2-b654669fd5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "def simulate_from_cov_matrix(cov_matrix, num_draws):\n",
    "    \n",
    "    # Calculate the eigenvalues and eigenvectors of the covariance matrix\n",
    "    eigenvalues, eigenvectors = np.linalg.eig(cov_matrix)\n",
    "\n",
    "    # Create a diagonal matrix from the eigenvalues\n",
    "    eigenvalue_matrix = np.diag(np.sqrt(eigenvalues))\n",
    "\n",
    "    # Simulate data from the covariance matrix\n",
    "    return np.dot(np.dot(eigenvectors, eigenvalue_matrix), np.random.normal(size=(cov_matrix.shape[0], num_draws)))\n",
    "\n",
    "def simulate_from_pca(data, num_draws, explained_variance=1.0):\n",
    "    # Fit the PCA model to the data\n",
    "    pca = PCA(n_components=data.shape[0], explained_variance=explained_variance)\n",
    "    pca.fit(data)\n",
    "\n",
    "    # Simulate data from the PCA model\n",
    "    return pca.inverse_transform(np.random.normal(size=(num_draws, data.shape[0])))\n",
    "\n",
    "def simulate_cov_matrix_direct_pca(cov_matrix, num_draws):\n",
    "    # Directly simulate data from the covariance matrix\n",
    "    direct = simulate_from_cov_matrix(cov_matrix, num_draws)\n",
    "\n",
    "    # Simulate data from the PCA model with 100% explained variance\n",
    "    pca_100 = simulate_from_pca(direct, num_draws, 1.0)\n",
    "\n",
    "    # Simulate data from the PCA model with 75% explained variance\n",
    "    pca_75 = simulate_from_pca(direct, num_draws, 0.75)\n",
    "\n",
    "    #\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87343f8f-e64c-4cb5-9858-3f94bf920c04",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_6998/18855419.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'DailyReturn.csv'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m','\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdrop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Unnamed: 0'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'SPY'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmeans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('DailyReturn.csv', sep=',')\n",
    "new_data = data.drop(columns=['Unnamed: 0', 'SPY'])\n",
    "new_data = new_data.iloc[:, 0:10]\n",
    "means = new_data.mean().values\n",
    "\n",
    "norm = (new_data - new_data.mean()).to_numpy()\n",
    "cov = norm.T @ norm\n",
    "\n",
    "alpha = 0.97\n",
    "weights = (1-alpha)**np.arange(len(new_data))[::-1]\n",
    "norm_new_data = (new_data - new_data.mean()).fillna(0).to_numpy()\n",
    "EW_cov = ((weights * norm_new_data.T)@norm_new_data)/(weights.sum())\n",
    "\n",
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "new_data_scale = scaler.fit_transform(new_data)\n",
    "new_data_pca = pca.fit(new_data_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fab7ec46-51ab-4e97-8af3-4c3482ed7aa3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
